{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d48cf8-cfae-4813-96ef-7e45109a614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "phase = 1\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # For multi-GPU setups, if any\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Example usage\n",
    "set_seed(58)  # Replace 42 with your chosen seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f998dc0-2720-478a-a2f3-539ec0f1e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalizaData(df):\n",
    "    # Convert DataFrame to PyTorch Tensor and send to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Normalizing Angular Data (Robotic Joints)\n",
    "    rad_cols = ['Tool_rx', 'Tool_ry', 'Tool_rz', 'Joint_base', 'Joint_Shoulder', 'Joint_Elbow', 'Joint_W1', 'Joint_W2', 'Joint_W3']\n",
    "    for col in rad_cols:\n",
    "        # Ensure the data is in the form of a PyTorch Tensor\n",
    "        rad_tensor = torch.tensor(df[col].values, device=device, dtype=torch.float32)\n",
    "        df[col + '_cos'] = torch.cos(rad_tensor).cpu().numpy()  # Compute cosine and convert back to NumPy for DataFrame\n",
    "        df[col + '_sin'] = torch.sin(rad_tensor).cpu().numpy()  # Compute sine and convert back to NumPy for DataFrame\n",
    "    \n",
    "    # Normalizing Linear Data (Speed, Acceleration, Energy Consumed, and Time)\n",
    "    # Min-Max Scaling: Scales the data to a fixed range, typically 0 to 1\n",
    "    scaler = MinMaxScaler()\n",
    "    linear_cols = ['Speed', 'Acceleration', 'Time', 'Energy_Consumped']\n",
    "    df[linear_cols] = scaler.fit_transform(df[linear_cols])\n",
    "    \n",
    "    return df, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed74d0-62f3-46db-b50e-a95dcf8f5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Load data\n",
    "real_data = pd.read_csv('phase RLHF dataset/remaining_paths.csv')\n",
    "\n",
    "\n",
    "# Fix a typo in the 'point_type' column\n",
    "real_data['point_type'] = real_data['point_type'].replace('approuch', 'approach')\n",
    "\n",
    "# Map 'point_type' and 'Movement Type' to numeric values\n",
    "points = {'pick': 1, 'approach': 2, 'start_point': 3, 'box1': 4, 'way_point': 5}\n",
    "movement_types = {'MoveL': 1, 'MoveJ': 2, 'Movel': 1, 'Movej': 2}\n",
    "real_data['point_type'] = real_data['point_type'].map(points)\n",
    "real_data['Movement Type'] = real_data['Movement Type'].map(movement_types)\n",
    "\n",
    "# Drop columns 'id' and 'row_index'\n",
    "real_data.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "# Assuming here that the use of PyTorch would be to handle numeric data for neural network processing\n",
    "# Convert DataFrame to PyTorch Tensor and send to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tensor_data = torch.tensor(real_data.values, dtype=torch.float, device=device)\n",
    "\n",
    "# Print number of columns in the DataFrame\n",
    "print(len(real_data.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163849c-cbb7-4e0e-a59d-ac85b9d79d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "df_normalized, scaler = normalizaData(real_data.copy())\n",
    "\n",
    "# Define the features to be used\n",
    "features = [\n",
    "    'point_type', 'Tool_x', 'Tool_y', 'Tool_z', 'Tool_rx_cos', 'Tool_rx_sin', 'Tool_ry_cos',\n",
    "    'Tool_ry_sin', 'Tool_rz_cos', 'Tool_rz_sin', 'Joint_base_cos',\n",
    "    'Joint_base_sin', 'Joint_Shoulder_cos', 'Joint_Shoulder_sin',\n",
    "    'Joint_Elbow_cos', 'Joint_Elbow_sin', 'Joint_W1_cos', 'Joint_W1_sin',\n",
    "    'Joint_W2_cos', 'Joint_W2_sin', 'Joint_W3_cos', 'Joint_W3_sin', 'Speed', 'Acceleration', 'Movement Type', 'Time', 'Energy_Consumped'\n",
    "]\n",
    "\n",
    "# Calculate the number of features\n",
    "features_num = len(features)\n",
    "\n",
    "# Select the specified features from the normalized DataFrame\n",
    "df_normalized = df_normalized[features]\n",
    "\n",
    "# Display the first few rows of the selected features\n",
    "df_normalized.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584db590-4351-423b-a038-624b160c5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_normalized has been defined and features_num has been calculated as before\n",
    "# Add a 'path_id' column to help in grouping. Creating a dummy 'path_id' that repeats every 5 rows\n",
    "num_paths = len(df_normalized) // 5  # Use integer division\n",
    "path_id = np.repeat(np.arange(num_paths), 5)\n",
    "df_normalized['path_id'] = path_id\n",
    "\n",
    "# Ensure the length of path_id matches the number of rows in df_normalized\n",
    "if len(path_id) < len(df_normalized):\n",
    "    extra_ids = np.array([num_paths] * (len(df_normalized) - len(path_id)))\n",
    "    path_id = np.concatenate([path_id, extra_ids])\n",
    "\n",
    "# Now, sort and group by 'path_id' and reshape\n",
    "grouped = df_normalized.groupby('path_id').apply(lambda x: x.iloc[:, :-1].values.reshape(1, 5, features_num))\n",
    "\n",
    "# Drop the 'path_id' column after reshaping\n",
    "df_normalized.drop(['path_id'], axis=1, inplace=True)\n",
    "\n",
    "# Concatenate the groups back into a single numpy array\n",
    "reshaped_data = np.concatenate(grouped.values, axis=0)\n",
    "\n",
    "# Save the reshaped data to a file. Assume 'phase' is defined elsewhere\n",
    "np.save(f\"real_normalized_dataset_shape_3D_ph{phase} WGAN.npy\", reshaped_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab78bf-b3ac-4a49-9b02-90be3dd3ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Setting the aesthetic style of the plots\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "col_16 = real_data['Time']\n",
    "summed_time = col_16.groupby(real_data.index // 5).sum()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(summed_time, kde=True, bins=10 , label = \"Time\")\n",
    "plt.title('Real Data - Distribution of Cycle time')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.savefig(\"Real Data - Distribution of Cycle time wgan\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea275190-56d5-416b-b82f-e74fd9dd423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Setting the aesthetic style of the plots\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "\n",
    "col_17 = real_data['Energy_Consumped']\n",
    "summed_energy_consumed = col_17.groupby(real_data.index // 5).sum()\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.histplot(summed_energy_consumed, kde=True, bins=10, label=\"Energy_Consumped\")\n",
    "plt.title('Real Data - Distribution of Energy Consumed')\n",
    "plt.xlabel('Energy Consumed')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.savefig(\"Real Data - Distribution of Energy Consumed wgan.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd2980a-47a6-4242-a980-0b5e51f9a234",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, num_features, sequence_length=5):\n",
    "        super(Generator, self).__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.num_features = num_features\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.fc_initial = nn.Sequential(\n",
    "            nn.Linear(self.noise_dim, 5 * self.num_features),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        # Define LSTM layers\n",
    "        self.lstm1 = nn.LSTM(self.num_features, 1024, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(1024, 256, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(256, 128, batch_first=True)\n",
    "        self.lstm4 = nn.LSTM(128, 64, batch_first=True)\n",
    "        self.lstm5 = nn.LSTM(64, 64, batch_first=True)\n",
    "\n",
    "        # Final fully connected layer\n",
    "        self.fc = nn.Linear(64, self.num_features)\n",
    "        self.relu = nn.ReLU()  # ReLU activation layer to use after LSTMs\n",
    "        self.tanh = nn.Tanh()  # Tanh activation for the final output\n",
    "\n",
    "    def forward(self, noise):\n",
    "        x = self.fc_initial(noise)\n",
    "        x = x.view(-1, self.sequence_length, self.num_features)\n",
    "        # Passing through each LSTM layer followed by ReLU activation\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.relu(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.relu(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.relu(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x = self.relu(x)\n",
    "        x, _ = self.lstm5(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Passing the final LSTM output through the fully connected layer followed by Tanh\n",
    "        x = x.contiguous().view(-1, 64)  # Flatten the output for the fully connected layer\n",
    "        print(x.shape, \"flatten\")\n",
    "        x = self.fc(x)\n",
    "        x = self.tanh(x)  # Applying Tanh after final FC\n",
    "\n",
    "        x = x.view(-1, self.sequence_length, self.num_features)\n",
    "        return x\n",
    "'''\n",
    "\n",
    "\n",
    "'''class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n",
    "        y = self.module(x_reshape)\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "        return y\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, num_features, sequence_length=5):\n",
    "        super(Generator, self).__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.num_features = num_features\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.fc_initial = nn.Sequential(\n",
    "            nn.Linear(self.noise_dim, 5 * self.num_features),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        # Define LSTM layers\n",
    "        self.lstm1 = nn.LSTM(self.num_features, 1024, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(1024, 256, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(256, 128, batch_first=True)\n",
    "        self.lstm4 = nn.LSTM(128, 64, batch_first=True)\n",
    "        self.lstm5 = nn.LSTM(64, 64, batch_first=True)\n",
    "\n",
    "        # TimeDistributed fully connected layer\n",
    "        self.fc_final = TimeDistributed(nn.Linear(64, self.num_features), batch_first=True)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, noise):\n",
    "        x = self.fc_initial(noise)\n",
    "        x = x.view(-1, self.sequence_length, self.num_features)\n",
    "\n",
    "        # Passing through each LSTM layer\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x, _ = self.lstm5(x)\n",
    "        \n",
    "        # Passing the final LSTM output through the TimeDistributed fully connected layer\n",
    "        x = self.fc_final(x)\n",
    "        x = self.tanh(x)  # Applying Tanh after final FC\n",
    "\n",
    "        return x\n",
    "\n",
    "# Example of initializing the Generator\n",
    "noise_dim = 100\n",
    "num_features = 27\n",
    "generator = Generator(noise_dim, num_features)\n",
    "noise = torch.randn(33, noise_dim)  # Example noise batch\n",
    "generated_data = generator(noise)\n",
    "print(generated_data.shape)  # Expected shape: [33, 5, 27]\n",
    "'''\n",
    "'''\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"An LSTM based generator. It expects a sequence of noise vectors as input.\n",
    "\n",
    "    Args:\n",
    "        in_dim: Input noise dimensionality\n",
    "        out_dim: Output dimensionality\n",
    "        n_layers: number of lstm layers\n",
    "        hidden_dim: dimensionality of the hidden layer of lstms\n",
    "\n",
    "    Input: noise of shape (batch_size, seq_len, in_dim)\n",
    "    Output: sequence of shape (batch_size, seq_len, out_dim)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, n_layers=5, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(in_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.linear = nn.Sequential(nn.Linear(hidden_dim, out_dim), nn.Tanh())\n",
    "        self.relu = nn.ReLU()  # ReLU activation layer to use after LSTMs\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, seq_len = input.size(0), input.size(1)\n",
    "        h_0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        c_0 = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "\n",
    "        recurrent_features, _ = self.lstm(input, (h_0, c_0))\n",
    "        outputs = self.linear(recurrent_features.contiguous().view(batch_size*seq_len, self.hidden_dim))\n",
    "        outputs = outputs.view(batch_size, seq_len, self.out_dim)'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b565e69-c3dd-4d11-9187-03ce3a2f5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, num_features, sequence_length=5):\n",
    "        super(Generator, self).__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.num_features = num_features\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.fc_initial = nn.Sequential(\n",
    "           # nn.Linear(self.noise_dim, 5 * self.num_features),\n",
    "            nn.Linear(self.noise_dim, 5 * self.num_features),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        # Define LSTM layers\n",
    "        self.lstm1 = nn.LSTM(self.num_features, 1024, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(1024, 256, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(256, 128, batch_first=True)\n",
    "        self.lstm4 = nn.LSTM(128, 64, batch_first=True)\n",
    "        self.lstm5 = nn.LSTM(64, 64, batch_first=True)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Linear(64, self.num_features),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.tanh = nn.Tanh()  # Tanh activation for the final output\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, noise):\n",
    "        x = self.fc_initial(noise)\n",
    "        x = x.view(-1, self.sequence_length, self.num_features)\n",
    "        # Passing through each LSTM layer followed by ReLU activation\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.relu(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.relu(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.relu(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x = self.relu(x)\n",
    "        x, _ = self.lstm5(x)\n",
    "        x = self.relu(x)\n",
    "         # Apply final transformation to each time point\n",
    "        x = self.final_layer(x)\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea69c4-6df8-46b7-aa34-7cebeb443d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sse_module(x):\n",
    "    \"\"\"\n",
    "    Spatial Squeeze and Excitation (SSE) module function for PyTorch.\n",
    "    \n",
    "    Parameters:\n",
    "    - x (torch.Tensor): The input tensor with dimensions (batch_size, channels, length).\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Output tensor after applying the SSE attention mechanism.\n",
    "    \"\"\"\n",
    "    # Ensure the input tensor is on the appropriate device (GPU if available)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    x = x.to(device)\n",
    "\n",
    "    # Convolution layer to expand feature dimensions\n",
    "    conv = nn.Conv1d(in_channels=x.size(1), out_channels=64, kernel_size=1, padding='same').to(device)\n",
    "    x = conv(x)\n",
    "    x = F.relu(x)\n",
    "\n",
    "    # Squeeze step: Global Average Pooling\n",
    "    squeeze = F.adaptive_avg_pool1d(x, 1).squeeze(-1)  # Global average pooling and removing the last dimension\n",
    "    \n",
    "    # Fully connected layer to generate attention weights\n",
    "    fc = nn.Linear(64, 64).to(device)\n",
    "    excitation = fc(squeeze)\n",
    "    excitation = torch.sigmoid(excitation)\n",
    "    \n",
    "    # Expand dimensions to match convolution output for multiplication\n",
    "    excitation = excitation.unsqueeze(-1)\n",
    "    \n",
    "    # Excitation step: Apply attention weights\n",
    "    out = x * excitation  # Element-wise multiplication\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7cf65-d190-425b-a3bc-f80522bdda3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, features_num):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.features_num = features_num\n",
    "        self.input_shape = (5, features_num)  # Shape of input data\n",
    "        self.lstm_input_size = features_num \n",
    "        # LSTM layers\n",
    "        self.lstm1 = nn.LSTM(self.lstm_input_size, 1024, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(1024, 256, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(256, 128, batch_first=True)\n",
    "        self.lstm4 = nn.LSTM(128, 32, batch_first=True)\n",
    "        self.lstm5 = nn.LSTM(32, 32, batch_first=True)\n",
    "        self.lstm6 = nn.LSTM(32, 32, batch_first=True)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Reshape condition to match input data for concatenation\n",
    "        # Concatenate input data and condition\n",
    "        merged_input = torch.cat([x], dim=2)\n",
    "\n",
    "        # Apply SSE module\n",
    "        #print(merged_input.shape, \"merged_input before\")\n",
    "        #merged_input = merged_input.permute(0, 1, 2)  # Change shape to (batch, channels, length)\n",
    "        #print(merged_input.shape, \"merged_input\")\n",
    "       # (None, 5, 29) merged_input\n",
    "\n",
    "        #sse_out = sse_module(merged_input)\n",
    "        #sse_out = sse_out.permute(0, 2, 1)  # Change back to (batch, length, channels)\n",
    "        #print(sse_out.shape,\"sse_out\")\n",
    "      \n",
    "        x, _ = self.lstm1(merged_input)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.dropout(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x = self.dropout(x)\n",
    "        x, _ = self.lstm5(x)\n",
    "        x, _ = self.lstm6(x)\n",
    "\n",
    "        # Take the output from the last LSTM step\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # Apply fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90f572-1668-4879-aca1-69fc76cbe35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_min_max(data_normalized, col_index ,col_index_realdata):\n",
    "    \"\"\"\n",
    "    Denormalize data normalized using Min-Max scaling.\n",
    "\n",
    "    Parameters:\n",
    "    - data_normalized (torch.Tensor): Normalized data tensor with shape (batch_size, seq_len, num_features).\n",
    "    - real_data_tensor (torch.Tensor): Original real data tensor used to determine min and max values.\n",
    "    - col_index (int): Index of the column to denormalize in the normalized data.\n",
    "    - col_index_realdata (int): Index of the corresponding column in the real data.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Denormalized column of data.\n",
    "    \n",
    "    \"\"\"\n",
    "    device = data_normalized.device  # Ensure the device is consistent\n",
    "    real_data_tensor = torch.tensor(real_data.to_numpy(), dtype=torch.float32).to(device)\n",
    "    # Calculate min and max values from the real data\n",
    "    min_vals = torch.min(real_data_tensor, dim=0)[0].to(device)  # Shape: (num_features,)\n",
    "    max_vals = torch.max(real_data_tensor, dim=0)[0].to(device)  # Shape: (num_features,)\n",
    "\n",
    "    # Extract the relevant min and max values for broadcasting\n",
    "    min_val = min_vals[col_index_realdata].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1)\n",
    "    max_val = max_vals[col_index_realdata].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1)\n",
    "\n",
    "    # Apply the denormalization formula\n",
    "    col_denormalized = data_normalized[:, :, col_index] * (max_val - min_val) + min_val\n",
    "\n",
    "    return col_denormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb2125-ddcc-464d-b4c3-1db80ee6f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_data(arr, scaler, features):\n",
    "    \n",
    "    \"\"\"\n",
    "    Denormalizes the data from a normalized array using the provided scaler.\n",
    "    \n",
    "    Parameters:\n",
    "    - arr (numpy.ndarray): The normalized array to be denormalized. Expected shape: (samples, features, timesteps).\n",
    "    - scaler (MinMaxScaler or StandardScaler): The scaler used to normalize the data.\n",
    "    - features (list): List of feature names for the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with denormalized data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reshape array into 2D (flatten the timesteps) and create DataFrame\n",
    "    reshaped_array = arr.reshape(-1, arr.shape[2])\n",
    "    df = pd.DataFrame(reshaped_array, columns=features)\n",
    "    \n",
    "    # Denormalizing Angular Data\n",
    "    # Identify columns related to cosine and sine representations\n",
    "    angle_columns = [col for col in df.columns if '_cos' in col or '_sin' in col]\n",
    "    \n",
    "    # Set to keep track of processed base columns to avoid duplicates\n",
    "    processed_bases = set()\n",
    "    \n",
    "    for col in angle_columns:\n",
    "        if '_cos' in col:\n",
    "            base_col = col.replace('_cos', '')\n",
    "            if base_col not in processed_bases:\n",
    "                # Combine cosine and sine to compute the angle\n",
    "                df[base_col] = np.arctan2(df[base_col + '_sin'], df[base_col + '_cos'])\n",
    "                # Remove the original cosine and sine columns\n",
    "                df.drop(columns=[base_col + '_sin', base_col + '_cos'], inplace=True)\n",
    "                # Mark the base column as processed\n",
    "                processed_bases.add(base_col)\n",
    "    \n",
    "    # Denormalizing Linear Data\n",
    "    # List of columns to denormalize\n",
    "    linear_columns = ['Speed', 'Acceleration', 'Time', 'Energy_Consumped']\n",
    "    \n",
    "    # Ensure all specified columns are in the DataFrame before attempting denormalization\n",
    "    available_linear_columns = [col for col in linear_columns if col in df.columns]\n",
    "    \n",
    "    if available_linear_columns:\n",
    "        df[available_linear_columns] = scaler.inverse_transform(df[available_linear_columns])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613cfa52-cacb-46fa-8f59-643c632eb4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def physics_loss(fake_data):\n",
    "    \"\"\"\n",
    "    Calculate the physics-based loss for generated data.\n",
    "\n",
    "    Parameters:\n",
    "    - fake_data (torch.Tensor): Generated data with shape (batch_size, seq_len, num_features).\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Total physics loss.\n",
    "    \"\"\"\n",
    "    fake_data_numpy = fake_data.detach().cpu().numpy()\n",
    "\n",
    "    df = denormalize_data(fake_data_numpy , scaler, features)\n",
    "    '''Index(['point_type', 'Tool_x', 'Tool_y', 'Tool_z', 'Speed', 'Acceleration',\n",
    "       'Movement Type', 'Time', 'Energy_Consumped', 'Tool_rx', 'Tool_ry',\n",
    "       'Tool_rz', 'Joint_base', 'Joint_Shoulder', 'Joint_Elbow', 'Joint_W1',\n",
    "       'Joint_W2', 'Joint_W3'],\n",
    "      dtype='object') '''\n",
    "    #Penalty for negative values in specific columns\n",
    "\n",
    "    #Movement type 1-> moveL , 2-> moveJ \n",
    "    #Tool x,y,z # Move l ,speed 0-3000 mm/s -> 0-3 m/s\n",
    "    #Tool x,y,z # Move l ,acceleration 0-150000 mm/s2 - 0-150 m/s \n",
    "    #Denormalize speed , 13 index in real data , 22 speed in normalized data\n",
    "    #[-25.2721, -25.7903, -25.5148, -25.1693, -24.9287],\n",
    "\n",
    "    # Speed constraints for MoveL\n",
    "    # i assume movment type = 1 if less than 0.5 \n",
    "    filtered_speed = len(df[(df['Speed'] >= 0) & (df['Speed'] <= 3) & (df['Movement Type'] <= 1)])\n",
    "    filtered_Acceleration = len(df[(df['Acceleration'] >= 0) & (df['Acceleration'] <= 150) & (df['Movement Type'] <= 1)])\n",
    "    speed_loss_l = filtered_speed\n",
    "    acc_loss_l = filtered_Acceleration\n",
    "  \n",
    "\n",
    "    # Speed constraints for MoveJ\n",
    "        #rx,ry,rz,joints \n",
    "        #Move J speed 0-180 degree per second -> radian 3.14159\n",
    "        #Move J acceleration 0 - 2292  degree/second ^2 -> radian 40\n",
    "    filtered_speed_mj = len(df[(df['Speed'] >= 0) & (df['Speed'] <=  3.14159) & (df['Movement Type'] >=1)])\n",
    "    filtered_Acceleration_mj = len(df[(df['Acceleration'] >= 0) & (df['Acceleration'] <= 40) & (df['Movement Type'] >= 1)])\n",
    "    speed_loss_j = filtered_speed_mj\n",
    "    acc_loss_j = filtered_Acceleration_mj\n",
    "  \n",
    "\n",
    "    # Joint angle constraints\n",
    "       #validate_joint_angles\n",
    "        #-363 to 363 degree/s ---->    -6.33555 , 6.33555 rad\n",
    "        #Joint limits as defined in the screenshot\n",
    "    joint_columns = ['Joint_base', 'Joint_Shoulder', 'Joint_Elbow', 'Joint_W1', 'Joint_W2', 'Joint_W3']\n",
    "    # Filter the DataFrame\n",
    "    filtered_df_joints = df[(df[joint_columns] < -6.33555) | (df[joint_columns] > 6.33555)].dropna(how='all', subset=joint_columns)\n",
    "    joints_loss = len(filtered_df_joints)\n",
    "   \n",
    "    #time, energy negative \n",
    "    filtered_df = df[(df['Time'] < 0) | (df['Energy_Consumped'] < 0)]\n",
    "    time_energy_loss = len(filtered_df)\n",
    "    total_loss = speed_loss_l + acc_loss_l + speed_loss_j + acc_loss_j + joints_loss + time_energy_loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ec292-18a0-4e57-8624-dc9bdc49f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_penalty(discriminator, real_data, fake_data,lambda_gp=10):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the gradient penalty for WGAN-GP with CuDNN disabled.\n",
    "\n",
    "    Parameters:\n",
    "    - discriminator (nn.Module): The discriminator model.\n",
    "    - real_data (torch.Tensor): Real data samples.\n",
    "    - fake_data (torch.Tensor): Generated data samples.\n",
    "    - device (torch.device): Device to run computations on (CPU or GPU).\n",
    "    - lambda_gp (float): Gradient penalty coefficient.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Gradient penalty term.\n",
    "    \"\"\"\n",
    "    batch_size = real_data.size(0)\n",
    "\n",
    "    # Interpolate between real and fake data\n",
    "    alpha = torch.rand(batch_size, 1, 1, device=device)\n",
    "    interpolates = alpha * real_data + (1 - alpha) * fake_data\n",
    "    interpolates.requires_grad_(True)\n",
    "\n",
    "    with torch.backends.cudnn.flags(enabled=False):\n",
    "        # Discriminator prediction for interpolates\n",
    "        prediction = discriminator(interpolates)\n",
    "\n",
    "        # Calculate gradients\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=prediction,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=torch.ones_like(prediction, device=device),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True\n",
    "        )[0]\n",
    "\n",
    "    # Flatten the gradients to (batch_size, -1) for norm calculation\n",
    "    gradients = gradients.reshape(batch_size, -1)\n",
    "\n",
    "    # Compute the L2 norm of the gradients\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = lambda_gp * ((gradient_norm - 1) ** 2).mean()\n",
    "\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f969b09-6fd1-451a-8436-b823f4f321e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Wasserstein loss for WGAN.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true (torch.Tensor): The ground truth labels, typically -1 for real and 1 for fake data.\n",
    "    - y_pred (torch.Tensor): The predictions from the discriminator.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: The computed Wasserstein loss.\n",
    "    \"\"\"\n",
    "    # Ensure that predictions and labels are on the same device (e.g., GPU)\n",
    "    device = y_pred.device  # Assuming y_pred is already on the correct device\n",
    "    y_true = y_true.to(device)\n",
    "\n",
    "    # Compute the Wasserstein loss as the negative mean of products\n",
    "    return -torch.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa044116-e22f-42a0-b6d9-93051587908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_noise(batch_size, noise_dim, seed=None):\n",
    "    \"\"\"\n",
    "    Generate uniform noise for the GAN.\n",
    "\n",
    "    Parameters:\n",
    "    - batch_size (int): Number of samples.\n",
    "    - noise_dim (int): Dimensionality of the noise vector.\n",
    "    - seed (int, optional): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Noise tensor.\n",
    "    \"\"\"\n",
    "    '''if seed is not None:\n",
    "        print(\"enter seed\")\n",
    "        torch.manual_seed(seed)'''\n",
    "    \n",
    "    return torch.randn(batch_size, noise_dim).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Define the shape of the tensor\n",
    "    #shape = (batch_size, 5, 27)\n",
    "    #torch.rand(batch_size, noise_dim) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b47db9-ae04-4eb6-87b7-1b8652b37b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def setup_gan(generator, discriminator, lr=0.00005):\n",
    "    \"\"\"\n",
    "    Setup the GAN by initializing optimizers and moving models to the appropriate device.\n",
    "\n",
    "    Parameters:\n",
    "    - generator (nn.Module): The generator model.\n",
    "    - discriminator (nn.Module): The discriminator model.\n",
    "    - lr (float): Learning rate for the optimizers.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of optimizers for generator and discriminator.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=lr)\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0001)\n",
    "\n",
    "    return optimizer_g, optimizer_d, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab8e1d0-1a37-4b24-8bd5-7b7380120ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def train_batches(generator, discriminator, optimizer_g, optimizer_d, device, epochs, batch_size, noise_dim,lambda_gp=20):\n",
    "    \"\"\"\n",
    "    Train the GAN using Wasserstein loss with gradient penalty.\n",
    "\n",
    "    Parameters:\n",
    "    - generator (nn.Module): The generator model.\n",
    "    - discriminator (nn.Module): The discriminator model.\n",
    "    - optimizer_g (torch.optim.Optimizer): Optimizer for the generator.\n",
    "    - optimizer_d (torch.optim.Optimizer): Optimizer for the discriminator.\n",
    "    - device (torch.device): Device to run computations on (CPU or GPU).\n",
    "    - epochs (int): Number of training epochs.\n",
    "    - batch_size (int): Size of each batch.\n",
    "    - noise_dim (int): Dimension of the noise vector.\n",
    "    - reshaped_data (numpy.ndarray): Real data samples.\n",
    "    - lambda_gp (float): Gradient penalty coefficient.\n",
    "    \"\"\"\n",
    "    reshaped_data_tensor = torch.tensor(reshaped_data, dtype=torch.float).to(device)\n",
    "\n",
    "    num_batches = int(np.ceil(len(reshaped_data_tensor) / batch_size))\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx in range(num_batches):\n",
    "            # Calculate start and end indices for the current batch\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(reshaped_data_tensor))\n",
    "\n",
    "            # Generate real data samples\n",
    "            real_data_batch = reshaped_data_tensor[start_idx:end_idx]\n",
    "\n",
    "            # Generate synthetic data samples\n",
    "            \n",
    "            noise = generate_noise(len(real_data_batch), noise_dim).to(device)\n",
    "            fake_data = generator(noise)\n",
    "            # === Train Discriminator ===\n",
    "            optimizer_d.zero_grad()\n",
    "            real_validity = discriminator(real_data_batch)\n",
    "            fake_validity = discriminator(fake_data.detach())\n",
    "\n",
    "            # Compute Wasserstein loss for discriminator\n",
    "            d_loss_real = wasserstein_loss(torch.ones_like(real_validity), real_validity)\n",
    "            d_loss_fake = wasserstein_loss(-torch.ones_like(fake_validity), fake_validity)\n",
    "\n",
    "            # Gradient penalty\n",
    "\n",
    "            gp =  gradient_penalty(discriminator, real_data_batch, fake_data)\n",
    "            d_loss = d_loss_real + d_loss_fake + gp\n",
    "            d_loss.backward()\n",
    "            optimizer_d.step()\n",
    "\n",
    "            # === Train Generator ===\n",
    "            optimizer_g.zero_grad()\n",
    "            # Generate fake data again for generator update\n",
    "            fake_data = generator(noise)\n",
    "            fake_validity = discriminator(fake_data)\n",
    "\n",
    "            # Compute Wasserstein loss for generator and physics loss\n",
    "            #g_loss=−E[D(fake samples)] the generator want D to give fake data max score , we want min problem so we add negative \n",
    "            g_loss = -wasserstein_loss(torch.ones_like(fake_validity), fake_validity) + physics_loss(fake_data)\n",
    "            g_loss.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "            # Optionally print the progress for each batch or save it\n",
    "            losses.append({\"epoch\": epoch, \"D Loss\": d_loss.item(), \"G Loss\": g_loss.item()})\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx+1}/{num_batches}, D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "def train(generator, discriminator, optimizer_g, optimizer_d, device, epochs, noise_dim, lambda_gp, param_grid):\n",
    "    \"\"\"\n",
    "    Train the GAN using Wasserstein loss with gradient penalty for the entire dataset at once.\n",
    "\n",
    "    Parameters:\n",
    "    - generator (nn.Module): The generator model.\n",
    "    - discriminator (nn.Module): The discriminator model.\n",
    "    - optimizer_g (torch.optim.Optimizer): Optimizer for the generator.\n",
    "    - optimizer_d (torch.optim.Optimizer): Optimizer for the discriminator.\n",
    "    - device (torch.device): Device to run computations on (CPU or GPU).\n",
    "    - epochs (int): Number of training epochs.\n",
    "    - noise_dim (int): Dimension of the noise vector.\n",
    "    - reshaped_data (numpy.ndarray): Real data samples.\n",
    "    - lambda_gp (float): Gradient penalty coefficient.\n",
    "    \"\"\"\n",
    "    tensor_data = torch.tensor(reshaped_data, dtype=torch.float32)\n",
    "    dataset = TensorDataset(tensor_data)\n",
    "    data_loader = DataLoader(dataset, batch_size=33, shuffle=True)\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "           for real_data_tensor, in data_loader:\n",
    "            real_data_tensor = real_data_tensor.to(device)\n",
    "            # Generate real data samples\n",
    "            real_data_batch = real_data_tensor\n",
    "            # Generate synthetic data samples\n",
    "            fake_data = []\n",
    "            for i in range(len(real_data_batch)):\n",
    "                noise = generate_noise(1, noise_dim).to(device)\n",
    "                fake_data_data = generator(noise)\n",
    "                fake_data.append(fake_data_data)\n",
    "                \n",
    "            fake_data = torch.cat(fake_data, dim=0).to(device)\n",
    "            \n",
    "            # === Train Discriminator ===\n",
    "            optimizer_d.zero_grad()\n",
    "            real_validity = discriminator(real_data_batch)\n",
    "            fake_validity = discriminator(fake_data.detach())\n",
    "    \n",
    "            # Compute Wasserstein loss for discriminator\n",
    "            d_loss_real = wasserstein_loss(torch.ones_like(real_validity), real_validity)\n",
    "            d_loss_fake = wasserstein_loss(-torch.ones_like(fake_validity), fake_validity)\n",
    "    \n",
    "            # Gradient penalty\n",
    "    \n",
    "            gp =  gradient_penalty(discriminator, real_data_batch, fake_data)\n",
    "            d_loss = d_loss_real + d_loss_fake + gp\n",
    "            d_loss.backward()\n",
    "            optimizer_d.step()\n",
    "    \n",
    "            # === Train Generator ===\n",
    "            optimizer_g.zero_grad()\n",
    "            # Generate fake data again for generator update\n",
    "            fake_data = []\n",
    "            for i in range(len(real_data_batch)):\n",
    "                noise = generate_noise(1, noise_dim).to(device)\n",
    "                fake_data_data = generator(noise)\n",
    "                fake_data.append(fake_data_data)\n",
    "            fake_data = torch.cat(fake_data, dim=0).to(device)\n",
    "            fake_validity = discriminator(fake_data)\n",
    "    \n",
    "            # Compute Wasserstein loss for generator and physics loss\n",
    "            phy_factor = param_grid\n",
    "            g_loss = -wasserstein_loss(torch.ones_like(fake_validity), fake_validity) + phy_factor * physics_loss(fake_data)\n",
    "            g_loss.backward()\n",
    "            optimizer_g.step()\n",
    "    \n",
    "            # Optionally print the progress for each batch or save it\n",
    "            losses.append({\"epoch\": epoch, \"D Loss\": d_loss.item(), \"G Loss\": g_loss.item()})\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n",
    "\n",
    "     \n",
    "    d_losses = [loss['D Loss'] for loss in losses]  # Assuming this contains total D loss\n",
    "    g_losses = [loss['G Loss'] for loss in losses]  # Assuming this contains G loss\n",
    "    \n",
    "    return generator, discriminator, np.mean(g_losses), losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc743f80-6dea-488a-bffd-933bfc80befd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "epochs = 5000\n",
    "batch_size = 33  # Set batch_size appropriately\n",
    "noise_dim = 100  # Dimensionality of the input noise vector\n",
    "num_features = 27\n",
    "\n",
    "generator = Generator(noise_dim, num_features)\n",
    "generator.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "discriminator = Discriminator(features_num)\n",
    "optimizer_g, optimizer_d, device = setup_gan(generator, discriminator, lr=0.0002)\n",
    "\n",
    "# Start training\n",
    "# Grid search\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_params = {}\n",
    "phy_factor_param = [0.2] #[0.2, 0.5, 1, 2 ]\n",
    "lambda_gp=10\n",
    "for value in phy_factor_param:\n",
    "    generator, discriminator, gloss, losses =train(generator, discriminator, optimizer_g, optimizer_d, device, epochs, noise_dim, lambda_gp, value)\n",
    "    if gloss < best_loss:\n",
    "        best_loss = gloss\n",
    "        best_params = {\"phy_factor_param\":value, \"generator_model\": generator, \"discriminator_model\": discriminator, \"gloss\": gloss, \"losses\":losses}\n",
    "        \n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c854519-40df-47a6-ae5c-e4443d8c3025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `generator` and `discriminator` are your model instances\n",
    "generator = best_params['generator_model']\n",
    "discriminator = best_params['discriminator_model']\n",
    "losses = best_params['losses']\n",
    "\n",
    "torch.save(generator.state_dict(), 'generator_model_phase1WGAN.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator_model_phase1WGAN.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5b32f1-ccc3-4c2b-8986-ea73c8dca54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302bd71-992a-4674-ad05-5fc12185b03a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66d9ec7-a6d9-45d6-8a36-397b28a21458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot \n",
    "\n",
    "# Generate fake data\n",
    "noise = generate_noise(len(real_data), noise_dim)\n",
    "# Random noise\n",
    "fake_data = generator(noise)\n",
    "\n",
    "dot = make_dot(generator(noise), params=dict(generator.named_parameters()))\n",
    "dot.render('generator_graph_WGAN', format='pdf')  # Saves the graph as a PNG file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21425084-5010-4829-9e39-0d9d3695c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `losses` is a list of dictionaries containing 'epoch', 'D Loss', and 'G Loss'\n",
    "# Extracting the data\n",
    "epochs_ls = [i for i in range(1, epochs + 1)]  # Integer epochs\n",
    "d_losses = [loss['D Loss'] for loss in losses]  # Assuming this contains total D loss\n",
    "g_losses = [loss['G Loss'] for loss in losses]  # Assuming this contains G loss\n",
    "print(np.mean(d_losses),\"d_losses\")\n",
    "print(np.mean(g_losses),\"g_losses\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_ls, d_losses, label='Discriminator Loss')\n",
    "plt.plot(epochs_ls, g_losses, label='Generator Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Losses Over Epochs')\n",
    "plt.legend()\n",
    "plt.savefig('Training_Losses_Over_Epochswgan.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d1b9d-7df3-4874-b7aa-d748629340d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(generator, sample_count, noise_dim,seed=None):\n",
    "    \"\"\"\n",
    "    Generate samples using the generator.\n",
    "\n",
    "    Parameters:\n",
    "    - generator (nn.Module): The generator model.\n",
    "    - sample_count (int): Total number of samples to generate.\n",
    "    - noise_dim (int): Dimension of the noise vector.\n",
    "    - seed (int, optional): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Generated samples tensor.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_size = len(reshaped_data)\n",
    "    all_samples = []\n",
    "\n",
    "    # Calculate number of full batches\n",
    "    num_full_batches = sample_count // batch_size\n",
    "\n",
    "    for i in range(num_full_batches):\n",
    "        # Generate noise for the batch\n",
    "        noise = generate_noise(batch_size, noise_dim).to(device)\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            samples = generator(noise).cpu()  # Move samples to CPU for concatenation\n",
    "        all_samples.append(samples)\n",
    "\n",
    "    # Handle the remainder if sample_count isn't a perfect multiple of batch_size\n",
    "    remainder = sample_count % batch_size\n",
    "    if remainder > 0:\n",
    "        noise = generate_noise(remainder, noise_dim, seed=seed).to(device)\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            samples = generator(noise).cpu()  # Move samples to CPU for concatenation\n",
    "\n",
    "        all_samples.append(samples)\n",
    "\n",
    "    # Concatenate all samples into a single tensor\n",
    "    return torch.cat(all_samples, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaaf4d9-b3dc-4d2c-9f2b-62043e71e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "noise_dim = 100 # This should match the dimension used during training\n",
    "sample_count = 1000\n",
    "\n",
    "# Generate samples using a for loop\n",
    "generated_samples = []\n",
    "for i in range(sample_count):\n",
    "    sample = generate_samples(generator, 1, noise_dim)\n",
    "    generated_samples.append(sample)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa320dd-f1c3-4aa1-b998-97a73041a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.onnx.export(generator,  generate_noise(1, noise_dim), \"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d376627-c4ea-4652-81a2-8cd19f6aff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the generated samples\n",
    "concatenated_tensor = torch.cat(generated_samples, dim=0)\n",
    "\n",
    "# Convert to NumPy array (if you need to save it as a .npy file)\n",
    "concatenated_array = concatenated_tensor.cpu().numpy()\n",
    "4# Save to file\n",
    "np.save(\"syntheticData_100Samplewgan.npy\", concatenated_array)\n",
    "concatenated_array.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65535d1a-ba7c-40f6-b050-d0585f41f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Selecting the last column from each series\n",
    "energy_col = concatenated_array[:, :, -1]\n",
    "energy_sums = np.sum(energy_col, axis=1)\n",
    "time_col = concatenated_array[:, :, -2]\n",
    "\n",
    "# Summing across the rows (time steps)\n",
    "time_sums = np.sum(time_col, axis=1)\n",
    "\n",
    "plt.scatter(time_sums, energy_sums, alpha=0.6)\n",
    "plt.title('Synthetic Paths - Distribution of Cycle Time and Energy')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Energy')\n",
    "plt.savefig('Synthetic Data - Distribution of Cycle Time and Energywgan.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27218dfa-3ad0-4174-92ae-61b804acaeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract normalized time and energy into separate variables\n",
    "denormalize_generated_data = denormalize_data(concatenated_array, scaler, features)\n",
    "denormalize_generated_data.to_csv(\"denormalized_sythData_phase1_waed_wgan.csv\")\n",
    "\n",
    "time_denorm = denormalize_generated_data['Time']  # Assuming time is the second last column\n",
    "energy_denorm = denormalize_generated_data['Energy_Consumped']  # Assuming energy is the last column\n",
    "\n",
    "# Ensure the series length is a multiple of 5\n",
    "n = 5\n",
    "length = len(time_denorm)\n",
    "trimmed_length = length - (length % n)\n",
    "# Trim the series if necessary\n",
    "trimmed_series = time_denorm[:trimmed_length]\n",
    "# Reshape to groups of 5\n",
    "reshaped_array = trimmed_series.to_numpy().reshape(-1, n)\n",
    "# Sum each group of 5\n",
    "time_denorm_sums = reshaped_array.sum(axis=1)\n",
    "trimmed_series = energy_denorm[:trimmed_length]\n",
    "# Reshape to groups of 5\n",
    "reshaped_array = energy_denorm.to_numpy().reshape(-1, n)\n",
    "# Sum each group of 5\n",
    "energy_denorm_sums = reshaped_array.sum(axis=1)\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(time_denorm_sums, energy_denorm_sums, color='blue')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Synthetic Paths - Distribution of Cycle Time and Energy')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Energy')\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig('denormalize-Synthetic Datawgan.pdf')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1984a678-b4ea-4fed-a511-b7f3b6fa64ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "last_col = real_data.iloc[:, 17]\n",
    "# Group by each set of five rows and sum\n",
    "# np.arange(len(last_col)) // 5 creates an integer index for each group of five\n",
    "energy_real = last_col.groupby(np.arange(len(last_col)) // 5).sum()\n",
    "last_col = real_data.iloc[:, 16]\n",
    "time_real = last_col.groupby(np.arange(len(last_col)) // 5).sum()\n",
    "\n",
    "plt.scatter(time_denorm_sums, energy_denorm_sums, color='blue',label='Synthetic Psths')\n",
    "plt.scatter(time_real, energy_real, color='green',label='Real Paths')\n",
    "\n",
    "plt.title('Synthetic & Real Paths- Distribution of Cycle time and Energy')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Energy')\n",
    "plt.legend()\n",
    "plt.savefig('Synthetic & Real Paths- Distribution of Cycle time and Energy wgan.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96829dd4-c44a-4b77-ad87-7809742906c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_16 = time_denorm_sums.flatten()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(col_16, kde=True, bins=10 , label = \"Time\")\n",
    "plt.title('Synthetic Data - Distribution of Cycle time')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.savefig(\"denormalized_data- Sythetic Data - Distribution of Cycle time wgan.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0539e9d5-b240-4c22-93f1-4f3ea9db4a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce80cf0-58ab-49ad-9def-aa79c663a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to determine the Pareto front for minimizing both dimensions\n",
    "def pareto_frontier_minimize_both(Xs, Ys):\n",
    "    sorted_points = sorted([[Xs[i], Ys[i]] for i in range(len(Xs))], key=lambda point: (point[0], point[1]))\n",
    "    pareto_front = [sorted_points[0]]\n",
    "    for current in sorted_points[1:]:\n",
    "        if current[1] < pareto_front[-1][1]:\n",
    "            pareto_front.append(current)\n",
    "    return [pair[0] for pair in pareto_front], [pair[1] for pair in pareto_front]\n",
    "\n",
    "# Compute Pareto front\n",
    "pareto_X, pareto_Y = pareto_frontier_minimize_both(time_denorm_sums, energy_denorm_sums)\n",
    "\n",
    "# Plotting the points and Pareto front\n",
    "plt.scatter(time_denorm_sums,energy_denorm_sums, color='blue', label='Sythetic Paths')  # Your original data points\n",
    "plt.scatter(time_real, energy_real, color='green', label='Real Paths')\n",
    "plt.plot(pareto_X, pareto_Y, color='red', linewidth=2.0, label='Pareto Front')  # Pareto front\n",
    "\n",
    "plt.title('Synthetic & Real Paths - Distribution of Cycle Time and Energy')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Energy')\n",
    "plt.legend()\n",
    "plt.savefig('Synthetic & Real Paths- Pareto Visulization wgan.pdf')\n",
    "plt.show()\n",
    "\n",
    "# Identifying the absolute optimal point (minimum energy and time)\n",
    "min_energy = min(pareto_Y)\n",
    "optimal_points = [(x, y) for x, y in zip(pareto_X, pareto_Y) if y == min_energy]\n",
    "print(\"Optimal Points (Min Energy and then Min Time):\", optimal_points)\n",
    "print(pareto_X, pareto_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a452ed13-2f62-413e-9ea5-555f6f23c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate random data points for Time and Energy\n",
    "np.random.seed(0)  # For reproducibility\n",
    "time = time_denorm_sums\n",
    "energy = energy_denorm_sums\n",
    "\n",
    "# Function to identify Pareto optimal points\n",
    "def identify_pareto(scores):\n",
    "    # Initialize a boolean array to identify Pareto points\n",
    "    is_pareto = np.ones(scores.shape[0], dtype=bool)\n",
    "    for i in range(scores.shape[0]):\n",
    "        for j in range(scores.shape[0]):\n",
    "            if all(scores[j] <= scores[i]) and any(scores[j] < scores[i]):\n",
    "                is_pareto[i] = False\n",
    "                break\n",
    "    return is_pareto\n",
    "\n",
    "# Identify Pareto front\n",
    "pareto_points = identify_pareto(np.c_[time, energy])\n",
    "pareto_time = time[pareto_points]\n",
    "pareto_energy = energy[pareto_points]\n",
    "\n",
    "# Ensure there are Pareto points before proceeding\n",
    "\n",
    "if pareto_time.size > 0 and pareto_energy.size > 0:\n",
    "    # Select the best point based on the shortest Euclidean distance from the origin\n",
    "    distances = np.sqrt(pareto_time**2 + pareto_energy**2)\n",
    "    best_index = np.argmin(distances)\n",
    "    best_time = pareto_time[best_index]\n",
    "    best_energy = pareto_energy[best_index]\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(time, energy, color='gray', label='Sythetic Paths')\n",
    "    plt.scatter(time_real, energy_real, color='green', label='Real Paths')\n",
    "\n",
    "    plt.scatter(pareto_time, pareto_energy, color='blue', label='Pareto Front')\n",
    "    plt.scatter(best_time, best_energy, color='red', label='Best Point')\n",
    "    plt.plot([0, best_time], [best_energy, best_energy], 'k--', lw=1)  # Horizontal line to Best Point\n",
    "    plt.plot([best_time, best_time], [0, best_energy], 'k--', lw=1)  # Vertical line to Best Point\n",
    "    plt.text(best_time, best_energy, ' Best Point', verticalalignment='bottom')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Energy')\n",
    "    plt.title('Pareto Front with Best Selected Point')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No Pareto optimal points were identified.\")\n",
    "print(best_time,best_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc3695-2858-4d73-8474-67b354e1341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "# Improvement calculation\n",
    "def improvement_ratio(real, generated):\n",
    "    time_improvement = (real[0] - generated[0]) / real[0] if real[0] != 0 else 0\n",
    "    energy_improvement = (real[1] - generated[1]) / real[1] if real[1] != 0 else 0\n",
    "    return time_improvement, energy_improvement\n",
    "\n",
    "# Example usage:\n",
    "real_point = (3.18, 58.5091)\n",
    "generated_point = (best_time, best_energy)\n",
    "\n",
    "\n",
    "# Get improvements\n",
    "time_improvement, energy_improvement = improvement_ratio(real_point, generated_point)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Time Improvement Ratio: {time_improvement:.2%}\")\n",
    "print(f\"Energy Improvement Ratio: {energy_improvement:.2%}\")\n",
    "generated_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d43766-6e9a-4684-a9a4-a69255a20fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_phase1=[]\n",
    "data = {\n",
    "    'Time': denormalize_generated_data['Time'],   # Replace 50 with the actual size\n",
    "    'Energy_Consumped': denormalize_generated_data['Energy_Consumped']\n",
    "}\n",
    "df_indexes = pd.DataFrame(data)\n",
    "\n",
    "# Group by every five rows and sum\n",
    "grouping_key = np.floor(np.arange(len(df_indexes)) / 5)\n",
    "\n",
    "grouped = df_indexes.groupby(grouping_key).sum()\n",
    "grouped['Index'] = grouped.index.astype(int) * 5  \n",
    "\n",
    "for t, e in zip(pareto_time, pareto_energy):\n",
    "    print(t,e)\n",
    "    try:\n",
    "        index = int(grouped[grouped['Time'] == t].index[0])\n",
    "        index_2 = int(grouped[grouped['Energy_Consumped'] == e].index[0])\n",
    "    except IndexError:\n",
    "        index = None\n",
    "        index_2 = None\n",
    "    if (index ==index_2):\n",
    "        indexes_phase1.append(index)\n",
    "\n",
    "# Convert to a DataFrame\n",
    "df_indexes = pd.DataFrame(indexes_phase1, columns=['path_index'])\n",
    "df_indexes.drop_duplicates(inplace=True)\n",
    "print(df_indexes)\n",
    "# Save to a CSV file\n",
    "df_indexes.to_csv('pathes_index_phase1.csv', index=False)\n",
    "denormalize_generated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5022757e-0f85-4cec-840c-f97b81888c98",
   "metadata": {},
   "source": [
    "# Phase 2  , Take the fav/accurate points from the engineer then re-fine-tune the generator "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b06586e-6a50-4d02-8b72-23994e7ecbfc",
   "metadata": {},
   "source": [
    "### Load Pre-trained GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd1da48-a9bb-44e5-a97d-b70938acf1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model weights\n",
    "generator.load_state_dict(torch.load('generator_model_phase1WGAN.pth'))\n",
    "discriminator.load_state_dict(torch.load('discriminator_model_phase1WGAN.pth'))\n",
    "\n",
    "optimizer_g, optimizer_d, device = setup_gan(generator, discriminator, lr=0.0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6328d9da-c4cb-4e7d-832b-c3d4f30f0839",
   "metadata": {},
   "source": [
    "### Define Human Feedback Mechanism\n",
    "Define the reinforcement learning components needed for HFRL, such as the policy network (generator) and the value network.\n",
    "\n",
    "Policy Network (Generator):\n",
    "\n",
    "\n",
    "The policy network in RL terminology is the component that decides which actions to take. In the context of a GAN, the generator acts as the policy network because it generates new samples (actions) based on some input noise.\n",
    "We reuse the pre-trained generator model for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0172f731-bfbf-4a58-b304-bfc2da84df72",
   "metadata": {},
   "source": [
    "### Value Network:\n",
    "\n",
    "\n",
    "The value network estimates the expected reward for a given state. In our context, it helps evaluate how good the generated samples are, based on the human feedback.\n",
    "The value network is a separate neural network that takes the same input as the generator and outputs a single value representing the expected reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fa3880-b8af-4659-a24a-6ffd8bb4aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_human_feedback(samples):\n",
    "    # Ensure samples are on GPU\n",
    "    samples = samples.to(device)\n",
    "    alpha = 0.5\n",
    "    feedback =[]\n",
    "    for sample in samples:\n",
    "        #lower values are better\n",
    "        rating = (sample[:,-2].sum() * alpha + sample[:,-1].sum() *(1-alpha))\n",
    "        feedback.append(float(rating))\n",
    "    \n",
    "    return torch.tensor(feedback, dtype=torch.float).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1927f-b216-498b-9f0c-cc538794c461",
   "metadata": {},
   "source": [
    "### 3. Define RL Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e7ada-15ea-42be-8608-bb43be605e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming `generator_ph1` is your policy network and already defined\n",
    "policy_network = generator\n",
    "\n",
    "# Define the value network in PyTorch\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_features):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(input_features, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "# Assume 'features' is defined elsewhere and provides the number of input features\n",
    "input_dim = 5 * len(features)  # Update with the actual size of 'features'\n",
    "value_network = ValueNetwork(input_dim).to(device)\n",
    "\n",
    "optimizer_value = optim.Adam(value_network.parameters(), lr=0.0002)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6235f2b7-7d02-4a17-ae51-60d41a8781f1",
   "metadata": {},
   "source": [
    "### 4. Modify GAN Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5bb4a4-0339-41e3-b536-0e2028f62bfc",
   "metadata": {},
   "source": [
    "Integrate the human feedback into the GAN training loop. Update the generator based on the human feedback using RL techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b1d8cc-f299-464f-9b06-730117f97f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_ph2 = []\n",
    "def train_ph2(generator, discriminator, optimizer_g, optimizer_d, device, epochs, noise_dim, hfrl_paths_normalized_3d, value_network, optimizer_value, lambda_gp=20):\n",
    "    for epoch in range(epochs):\n",
    "        real_data_batch = torch.from_numpy(hfrl_paths_normalized_3d).float().to(device)\n",
    "\n",
    "        noise = generate_noise(len(real_data_batch), noise_dim).to(device)\n",
    "        fake_data = generator(noise)\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_d.zero_grad()\n",
    "        real_validity = discriminator(real_data_batch)\n",
    "        fake_validity = discriminator(fake_data.detach())\n",
    "        d_loss_real = wasserstein_loss(torch.ones_like(real_validity), real_validity)\n",
    "        d_loss_fake = wasserstein_loss(-torch.ones_like(fake_validity), fake_validity)\n",
    "        gp = gradient_penalty(discriminator, real_data_batch, fake_data)\n",
    "        d_loss = d_loss_real + d_loss_fake + lambda_gp * gp\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        optimizer_d.step()\n",
    "\n",
    "        \n",
    "        \n",
    "        # Update Value Network\n",
    "        feedback = collect_human_feedback(fake_data)\n",
    "        optimizer_value.zero_grad()\n",
    "        mse_loss = nn.MSELoss()\n",
    "        if isinstance(fake_data, list):\n",
    "            fake_data = torch.tensor(fake_data, dtype=torch.float).to(device)  # Convert list to tensor\n",
    "        predicted_feedback = value_network(fake_data)\n",
    "        feedback = feedback.view(predicted_feedback.shape)\n",
    "        value_loss = mse_loss(feedback, predicted_feedback)\n",
    "        value_loss.backward(retain_graph=True)\n",
    "        optimizer_value.step()\n",
    "\n",
    "         # Train Generator\n",
    "        optimizer_g.zero_grad()\n",
    "        fake_data = generator(noise)\n",
    "        fake_validity = discriminator(fake_data)\n",
    "        phy_factor = 0.5\n",
    "        phy_loss = phy_factor * physics_loss(fake_data)\n",
    "        g_loss = -wasserstein_loss(torch.ones_like(fake_validity), fake_validity)\n",
    "        \n",
    "        #lower values are better\n",
    "        #modify the generator's loss to include this feedback negatively (since you typically minimize loss)\n",
    "        val_factor = 1\n",
    "        phy_factor = 0.5\n",
    "        total_g_loss = g_loss + val_factor * value_loss.detach() + phy_factor * phy_loss  # where lambda is a weighting factor\n",
    "        total_g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        losses_ph2.append({\"epoch\": epoch, \"D Loss\": d_loss.item(), \"G Loss\": total_g_loss.item(), \"Value Loss\": value_loss.item()})\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, D Loss: {d_loss.item()}, G Loss: {total_g_loss.item()}, Value Loss: {value_loss.item()}\")\n",
    "\n",
    "    return losses_ph2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e6d116-3b63-490f-87b4-dd21876f4aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hfrl_paths = pd.read_csv(\"waeeeed paths after engineer review it.csv\")\n",
    "hfrl_paths = pd.read_csv(\"phase RLHF dataset/least_energy_time_paths.csv\")\n",
    "\n",
    "# Fix a typo in the 'point_type' column\n",
    "real_data['point_type'] = hfrl_paths['point_type'].replace('approuch', 'approach')\n",
    "\n",
    "# Map 'point_type' and 'Movement Type' to numeric values\n",
    "points = {'pick': 1, 'approach': 2, 'start_point': 3, 'box1': 4, 'way_point': 5}\n",
    "movement_types = {'MoveL': 1, 'MoveJ': 2, 'Movel': 1, 'Movej': 2}\n",
    "hfrl_paths['point_type'] = hfrl_paths['point_type'].map(points)\n",
    "hfrl_paths['Movement Type'] = hfrl_paths['Movement Type'].map(movement_types)\n",
    "\n",
    "# Drop columns 'id' and 'row_index'\n",
    "hfrl_paths.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "hfrl_paths_normalized, scaler_ph2 = normalizaData(hfrl_paths)\n",
    "\n",
    "#exclude the  Joint_W3 and keep sin , cos\n",
    "hfrl_paths_normalized= hfrl_paths_normalized[features]\n",
    "num_paths = len(hfrl_paths) // 5\n",
    "batch_size = num_paths #all data \n",
    "\n",
    "path_id = np.repeat(np.arange(num_paths), 5)\n",
    "hfrl_paths_normalized['path_id'] = path_id\n",
    "grouped = hfrl_paths_normalized.groupby('path_id').apply(lambda x: x.values[:, :-1].reshape(1, 5,features_num))\n",
    "hfrl_paths_normalized.drop(['path_id'], axis=1, inplace=True)\n",
    "\n",
    "# Concatenate the groups back into a single numpy array\n",
    "hfrl_paths_normalized_3d = np.concatenate(grouped.values, axis=0)\n",
    "\n",
    "optimizer_g, optimizer_d, device = setup_gan(generator, discriminator, lr=0.0002)\n",
    "losses_ph2 =  train_ph2(generator, discriminator, optimizer_g, optimizer_d, device, epochs, noise_dim, hfrl_paths_normalized_3d, value_network ,optimizer_value,  lambda_gp=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abdfb05-88d2-47eb-8d0b-3ae2bfb0313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the data\n",
    "\n",
    "#Epoch: 41, Batch: 34, D Loss: 0.7058187127113342, G Loss: 0.6761125326156616, Value Loss: 9.605087280273438\n",
    "\n",
    "epochs_ls = [loss['epoch'] for loss in losses_ph2]\n",
    "d_losses = [loss['D Loss'] for loss in losses_ph2]\n",
    "g_losses = [loss['G Loss'] for loss in losses_ph2]\n",
    "value_losses =  [loss['Value Loss'] for loss in losses_ph2]\n",
    "d_losses_real = []\n",
    "d_losses_fake = []\n",
    "\n",
    "print(np.mean(d_losses),\"d_losses\")\n",
    "print(np.mean(g_losses),\"g_losses\")\n",
    "print(np.mean(value_losses),\"reward\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "#d_loss_real\n",
    "plt.plot(epochs_ls, d_losses, label='Discriminator Loss')\n",
    "\n",
    "plt.plot(epochs_ls, g_losses, label='Generator Loss')\n",
    "plt.plot(epochs_ls, value_losses, label='Reward Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Losses Over Epochs')\n",
    "plt.legend()\n",
    "plt.savefig('Training Losses Over Epochs wgan.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34503e8-f0a1-497f-84b0-b0cabf58b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "noise_dim = 100  # This should match the dimension used during training\n",
    "sample_count = 1000\n",
    "\n",
    "# Generate samples using a for loop\n",
    "samples_syth = []\n",
    "for i in range(sample_count):\n",
    "    sample = generate_samples(generator, 1, noise_dim)\n",
    "    samples_syth.append(sample)\n",
    "    \n",
    "samples_syth = torch.cat(samples_syth, dim=0)\n",
    "\n",
    "# Convert to NumPy array (if you need to save it as a .npy file)\n",
    "samples_syth = samples_syth.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7c470b-5543-497e-905c-67a8bdba4147",
   "metadata": {},
   "source": [
    "### Denormalize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ca9868-fc3f-41b9-a60e-c70adb926bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_syth_ph2 = denormalize_data(samples_syth, scaler_ph2,features)\n",
    "samples_syth_ph2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5f26c4-13a5-40c0-9347-c08b4d61004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract normalized time and energy into separate variables\n",
    "denormalize_generated_data = denormalize_data(concatenated_array, scaler, features)\n",
    "\n",
    "time_denorm = samples_syth_ph2['Time']  # Assuming time is the second last column\n",
    "energy_denorm = samples_syth_ph2['Energy_Consumped']  # Assuming energy is the last column\n",
    "\n",
    "# Ensure the series length is a multiple of 5\n",
    "n = 5\n",
    "length = len(time_denorm)\n",
    "trimmed_length = length - (length % n)\n",
    "# Trim the series if necessary\n",
    "trimmed_series = time_denorm[:trimmed_length]\n",
    "# Reshape to groups of 5\n",
    "reshaped_array = trimmed_series.to_numpy().reshape(-1, n)\n",
    "# Sum each group of 5\n",
    "time_denorm_sums = reshaped_array.sum(axis=1)\n",
    "trimmed_series = energy_denorm[:trimmed_length]\n",
    "# Reshape to groups of 5\n",
    "reshaped_array = energy_denorm.to_numpy().reshape(-1, n)\n",
    "# Sum each group of 5\n",
    "energy_denorm_sums = reshaped_array.sum(axis=1)\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(time_denorm_sums, energy_denorm_sums, color='blue')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Synthetic Paths - Distribution of Cycle Time and Energy')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Energy')\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig('denormalize-Synthetic Data.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f85189-23f6-488d-b112-73b684fbb733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data \n",
    "time = samples_syth_ph2['Time']\n",
    "energy = samples_syth_ph2['Energy_Consumped']\n",
    "\n",
    "# Function to determine the Pareto front for minimizing both dimensions\n",
    "def pareto_frontier_minimize_both(Xs, Ys):\n",
    "    sorted_points = sorted([[Xs[i], Ys[i]] for i in range(len(Xs))], key=lambda point: (point[0], point[1]))\n",
    "    pareto_front = [sorted_points[0]]\n",
    "    for current in sorted_points[1:]:\n",
    "        if current[1] < pareto_front[-1][1]:\n",
    "            pareto_front.append(current)\n",
    "    return [pair[0] for pair in pareto_front], [pair[1] for pair in pareto_front]\n",
    "\n",
    "# Compute Pareto front\n",
    "pareto_X, pareto_Y = pareto_frontier_minimize_both(time, energy)\n",
    "\n",
    "# Plotting the points and Pareto front\n",
    "plt.scatter(time, energy, color='blue', label='Sythetic Paths')  # Your original data points\n",
    "plt.scatter(time_real, energy_real, color='green', label='Real Paths')\n",
    "plt.plot(pareto_X, pareto_Y, color='red', linewidth=2.0, label='Pareto Front')  # Pareto front\n",
    "\n",
    "plt.title('Synthetic & Real Paths - Distribution of Cycle Time and Energy')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Energy')\n",
    "plt.legend()\n",
    "plt.savefig('Synthetic & Real Paths- Pareto Visulization wgan.pdf')\n",
    "plt.show()\n",
    "\n",
    "# Identifying the a-bsolute optimal point (minimum energy and time)\n",
    "min_energy = min(pareto_Y)\n",
    "optimal_points = [(x, y) for x, y in zip(pareto_X, pareto_Y) if y == min_energy]\n",
    "print(\"Optimal Points (Min Energy and then Min  Time):\", optimal_points)\n",
    "print(pareto_X, pareto_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa96bdb0-2932-4685-9d77-5bdb22e98f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = time_denorm_sums\n",
    "energy =energy_denorm_sums\n",
    "\n",
    "\n",
    "\n",
    "# Function to identify Pareto optimal points\n",
    "def identify_pareto(scores):\n",
    "    # Initialize a boolean array to identify Pareto points\n",
    "    is_pareto = np.ones(scores.shape[0], dtype=bool)\n",
    "    for i in range(scores.shape[0]):\n",
    "        for j in range(scores.shape[0]):\n",
    "            if all(scores[j] <= scores[i]) and any(scores[j] < scores[i]):\n",
    "                is_pareto[i] = False\n",
    "                break\n",
    "    return is_pareto\n",
    "\n",
    "# Identify Pareto front\n",
    "pareto_points = identify_pareto(np.c_[time, energy])\n",
    "pareto_time = time[pareto_points]\n",
    "pareto_energy = energy[pareto_points]\n",
    "\n",
    "# Ensure there are Pareto points before proceeding\n",
    "\n",
    "if pareto_time.size > 0 and pareto_energy.size > 0:\n",
    "    # Select the best point based on the shortest Euclidean distance from the origin\n",
    "    distances = np.sqrt(pareto_time**2 + pareto_energy**2)\n",
    "    best_index = np.argmin(distances)\n",
    "    best_time = pareto_time[best_index]\n",
    "    best_energy = pareto_energy[best_index]\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(time, energy, color='gray', label='Sythetic Paths')\n",
    "    plt.scatter(time_real, energy_real, color='green', label='Real Paths')\n",
    "\n",
    "    plt.scatter(pareto_time, pareto_energy, color='blue', label='Pareto Front')\n",
    "    plt.scatter(best_time, best_energy, color='red', label='Best Point')\n",
    "    plt.plot([0, best_time], [best_energy, best_energy], 'k--', lw=1)  # Horizontal line to Best Point\n",
    "    plt.plot([best_time, best_time], [0, best_energy], 'k--', lw=1)  # Vertical line to Best Point\n",
    "    plt.text(best_time, best_energy, ' Best Point', verticalalignment='bottom')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Energy')\n",
    "    plt.title('Pareto Front with Best Selected Point')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No Pareto optimal points were identified.\")\n",
    "print(best_time,best_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadf438c-1970-4777-a75e-a8d2c7983aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "# Improvement calculation\n",
    "def improvement_ratio(real, generated):\n",
    "    time_improvement = (real[0] - generated[0]) / real[0] if real[0] != 0 else 0\n",
    "    energy_improvement = (real[1] - generated[1]) / real[1] if real[1] != 0 else 0\n",
    "    return time_improvement, energy_improvement\n",
    "\n",
    "# Example usage:\n",
    "real_point = (3.18, 58.5091)\n",
    "generated_point = (best_time, best_energy)\n",
    "\n",
    "\n",
    "# Get improvements\n",
    "time_improvement, energy_improvement = improvement_ratio(real_point, generated_point)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Time Improvement Ratio: {time_improvement:.2%}\")\n",
    "print(f\"Energy Improvement Ratio: {energy_improvement:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6dcd4-b711-4e46-9b63-0e5e00a17182",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx onnx-tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b66c20-002b-4600-bf27-866a1d28122c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
